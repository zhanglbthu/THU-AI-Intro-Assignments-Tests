{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Data Mining on Sentiment Analysis"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["---"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Preliminaries"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Import libraries. **You can add other libraries if necessary.**"]},{"cell_type":"code","execution_count":47,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-10-16T23:37:03.83871Z","iopub.status.busy":"2022-10-16T23:37:03.837369Z","iopub.status.idle":"2022-10-16T23:37:04.798601Z","shell.execute_reply":"2022-10-16T23:37:04.79744Z","shell.execute_reply.started":"2022-10-16T23:37:03.838618Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer  # Data transformation\n","from sklearn.model_selection import train_test_split  # Data testing\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score  # Comparison between real and predicted\n","import re  # Regular expressions\n","import nltk\n","from nltk import word_tokenize\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","from collections import Counter\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import GridSearchCV # 交叉验证\n","import os\n","import time"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Load the data and add column keys."]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2022-10-16T23:37:04.804857Z","iopub.status.busy":"2022-10-16T23:37:04.804516Z","iopub.status.idle":"2022-10-16T23:37:05.043216Z","shell.execute_reply":"2022-10-16T23:37:05.042013Z","shell.execute_reply.started":"2022-10-16T23:37:04.804825Z"},"trusted":true},"outputs":[],"source":["train_data = pd.read_csv(\"data/sentiment_train.csv\", header=None)\n","test_data = pd.read_csv(\"data/sentiment_test.csv\", header=None)\n","train_data.columns = ['id', 'information', 'type', 'text']\n","test_data.columns = ['id', 'information', 'type', 'text']"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Take a glance at the provided data."]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2022-10-16T23:37:05.044993Z","iopub.status.busy":"2022-10-16T23:37:05.044632Z","iopub.status.idle":"2022-10-16T23:37:05.062473Z","shell.execute_reply":"2022-10-16T23:37:05.06133Z","shell.execute_reply.started":"2022-10-16T23:37:05.044962Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 74682 entries, 0 to 74681\n","Data columns (total 4 columns):\n"," #   Column       Non-Null Count  Dtype \n","---  ------       --------------  ----- \n"," 0   id           74682 non-null  int64 \n"," 1   information  74682 non-null  object\n"," 2   type         74682 non-null  object\n"," 3   text         73996 non-null  object\n","dtypes: int64(1), object(3)\n","memory usage: 2.3+ MB\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>information</th>\n","      <th>type</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2401</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>im getting on borderlands and i will murder yo...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2401</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>I am coming to the borders and I will kill you...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2401</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>im getting on borderlands and i will kill you ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2401</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>im coming on borderlands and i will murder you...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2401</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>im getting on borderlands 2 and i will murder ...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>2401</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>im getting into borderlands and i can murder y...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>2402</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>So I spent a few hours making something for fu...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>2402</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>So I spent a couple of hours doing something f...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>2402</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>So I spent a few hours doing something for fun...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>2402</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>So I spent a few hours making something for fu...</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>2402</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>2010 So I spent a few hours making something f...</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>2402</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>was</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>2403</td>\n","      <td>Borderlands</td>\n","      <td>Neutral</td>\n","      <td>Rock-Hard La Varlope, RARE &amp; POWERFUL, HANDSOM...</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>2403</td>\n","      <td>Borderlands</td>\n","      <td>Neutral</td>\n","      <td>Rock-Hard La Varlope, RARE &amp; POWERFUL, HANDSOM...</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>2403</td>\n","      <td>Borderlands</td>\n","      <td>Neutral</td>\n","      <td>Rock-Hard La Varlope, RARE &amp; POWERFUL, HANDSOM...</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>2403</td>\n","      <td>Borderlands</td>\n","      <td>Neutral</td>\n","      <td>Rock-Hard La Vita, RARE BUT POWERFUL, HANDSOME...</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>2403</td>\n","      <td>Borderlands</td>\n","      <td>Neutral</td>\n","      <td>Live Rock - Hard music La la Varlope, RARE &amp; t...</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>2403</td>\n","      <td>Borderlands</td>\n","      <td>Neutral</td>\n","      <td>I-Hard like me, RARE LONDON DE, HANDSOME 2011,...</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>2404</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>that was the first borderlands session in a lo...</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>2404</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>this was the first Borderlands session in a lo...</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>2404</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>that was the first borderlands session in a lo...</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>2404</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>that was the first borderlands session in a lo...</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>2404</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>that I was the first real borderlands session ...</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>2404</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>that was the first borderlands session in a ho...</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>2405</td>\n","      <td>Borderlands</td>\n","      <td>Negative</td>\n","      <td>the biggest dissappoinment in my life came out...</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>2405</td>\n","      <td>Borderlands</td>\n","      <td>Negative</td>\n","      <td>The biggest disappointment of my life came a y...</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>2405</td>\n","      <td>Borderlands</td>\n","      <td>Negative</td>\n","      <td>The biggest disappointment of my life came a y...</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>2405</td>\n","      <td>Borderlands</td>\n","      <td>Negative</td>\n","      <td>the biggest dissappoinment in my life coming o...</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>2405</td>\n","      <td>Borderlands</td>\n","      <td>Negative</td>\n","      <td>For the biggest male dissappoinment in my life...</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>2405</td>\n","      <td>Borderlands</td>\n","      <td>Negative</td>\n","      <td>the biggest dissappoinment in my life came bac...</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>2406</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>WE FINISHED BORDERLANDS 3 FINALLY YAS! Thank y...</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>2406</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>WE FINALLY FINALLY FIND BORDERLANDS 3 YES! Tha...</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>2406</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>Thank you for hanging up everyone! It was fun....</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>2406</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>WE FINISHED BORDERLANDS 3 UPDATE YAS! Thank yo...</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>2406</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>WE FINISHED BORDERLANDS 3 AND FINALLY YAS! Tha...</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>2406</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>WE FINISHED BORDERLANDS 3 FINALLY YAS! Hey you...</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>2407</td>\n","      <td>Borderlands</td>\n","      <td>Negative</td>\n","      <td>Man Gearbox really needs to fix this dissapoin...</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>2407</td>\n","      <td>Borderlands</td>\n","      <td>Negative</td>\n","      <td>Man Gearbox really needs to fix these disappoi...</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>2407</td>\n","      <td>Borderlands</td>\n","      <td>Negative</td>\n","      <td>Man Gearbox really needs to fix this disssapoi...</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>2407</td>\n","      <td>Borderlands</td>\n","      <td>Negative</td>\n","      <td>Man Bethesda really needs to fix this dissapoi...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      id  information      type   \n","0   2401  Borderlands  Positive  \\\n","1   2401  Borderlands  Positive   \n","2   2401  Borderlands  Positive   \n","3   2401  Borderlands  Positive   \n","4   2401  Borderlands  Positive   \n","5   2401  Borderlands  Positive   \n","6   2402  Borderlands  Positive   \n","7   2402  Borderlands  Positive   \n","8   2402  Borderlands  Positive   \n","9   2402  Borderlands  Positive   \n","10  2402  Borderlands  Positive   \n","11  2402  Borderlands  Positive   \n","12  2403  Borderlands   Neutral   \n","13  2403  Borderlands   Neutral   \n","14  2403  Borderlands   Neutral   \n","15  2403  Borderlands   Neutral   \n","16  2403  Borderlands   Neutral   \n","17  2403  Borderlands   Neutral   \n","18  2404  Borderlands  Positive   \n","19  2404  Borderlands  Positive   \n","20  2404  Borderlands  Positive   \n","21  2404  Borderlands  Positive   \n","22  2404  Borderlands  Positive   \n","23  2404  Borderlands  Positive   \n","24  2405  Borderlands  Negative   \n","25  2405  Borderlands  Negative   \n","26  2405  Borderlands  Negative   \n","27  2405  Borderlands  Negative   \n","28  2405  Borderlands  Negative   \n","29  2405  Borderlands  Negative   \n","30  2406  Borderlands  Positive   \n","31  2406  Borderlands  Positive   \n","32  2406  Borderlands  Positive   \n","33  2406  Borderlands  Positive   \n","34  2406  Borderlands  Positive   \n","35  2406  Borderlands  Positive   \n","36  2407  Borderlands  Negative   \n","37  2407  Borderlands  Negative   \n","38  2407  Borderlands  Negative   \n","39  2407  Borderlands  Negative   \n","\n","                                                 text  \n","0   im getting on borderlands and i will murder yo...  \n","1   I am coming to the borders and I will kill you...  \n","2   im getting on borderlands and i will kill you ...  \n","3   im coming on borderlands and i will murder you...  \n","4   im getting on borderlands 2 and i will murder ...  \n","5   im getting into borderlands and i can murder y...  \n","6   So I spent a few hours making something for fu...  \n","7   So I spent a couple of hours doing something f...  \n","8   So I spent a few hours doing something for fun...  \n","9   So I spent a few hours making something for fu...  \n","10  2010 So I spent a few hours making something f...  \n","11                                                was  \n","12  Rock-Hard La Varlope, RARE & POWERFUL, HANDSOM...  \n","13  Rock-Hard La Varlope, RARE & POWERFUL, HANDSOM...  \n","14  Rock-Hard La Varlope, RARE & POWERFUL, HANDSOM...  \n","15  Rock-Hard La Vita, RARE BUT POWERFUL, HANDSOME...  \n","16  Live Rock - Hard music La la Varlope, RARE & t...  \n","17  I-Hard like me, RARE LONDON DE, HANDSOME 2011,...  \n","18  that was the first borderlands session in a lo...  \n","19  this was the first Borderlands session in a lo...  \n","20  that was the first borderlands session in a lo...  \n","21  that was the first borderlands session in a lo...  \n","22  that I was the first real borderlands session ...  \n","23  that was the first borderlands session in a ho...  \n","24  the biggest dissappoinment in my life came out...  \n","25  The biggest disappointment of my life came a y...  \n","26  The biggest disappointment of my life came a y...  \n","27  the biggest dissappoinment in my life coming o...  \n","28  For the biggest male dissappoinment in my life...  \n","29  the biggest dissappoinment in my life came bac...  \n","30  WE FINISHED BORDERLANDS 3 FINALLY YAS! Thank y...  \n","31  WE FINALLY FINALLY FIND BORDERLANDS 3 YES! Tha...  \n","32  Thank you for hanging up everyone! It was fun....  \n","33  WE FINISHED BORDERLANDS 3 UPDATE YAS! Thank yo...  \n","34  WE FINISHED BORDERLANDS 3 AND FINALLY YAS! Tha...  \n","35  WE FINISHED BORDERLANDS 3 FINALLY YAS! Hey you...  \n","36  Man Gearbox really needs to fix this dissapoin...  \n","37  Man Gearbox really needs to fix these disappoi...  \n","38  Man Gearbox really needs to fix this disssapoi...  \n","39  Man Bethesda really needs to fix this dissapoi...  "]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["train_data.info()\n","train_data.head(40)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Each item consists of 4 columns, where Columns ID and Information are almost task-irrevalent. **Column Type is users' sentiments, which we should predict as our labels based on Column Text.**\n","Now let's take a look at possible values of Column Type."]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"data":{"text/plain":["type\n","Negative      22542\n","Positive      20832\n","Neutral       18318\n","Irrelevant    12990\n","Name: count, dtype: int64"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["train_data[\"type\"].value_counts()"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"data":{"text/plain":["type\n","Neutral       285\n","Positive      277\n","Negative      266\n","Irrelevant    172\n","Name: count, dtype: int64"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["test_data[\"type\"].value_counts()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["There are 4 possible values, and ***our goal is to perform the quadruple classification over texts.***"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["---"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Data Processing"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["def preprocess(text):\n","    # TODO\n","    # 将输入文本转换为字符串类型\n","    text = str(text)\n","    # 将所有字母转换为小写字母\n","    text = text.lower()\n","    # 过滤文本中的标点符号和空格\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    return text  # modify this line"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Perform preprocessing, and compare the raw text and the preprocessed tokens."]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2022-10-16T23:37:05.121422Z","iopub.status.busy":"2022-10-16T23:37:05.121009Z","iopub.status.idle":"2022-10-16T23:37:05.59644Z","shell.execute_reply":"2022-10-16T23:37:05.595292Z","shell.execute_reply.started":"2022-10-16T23:37:05.121385Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>information</th>\n","      <th>type</th>\n","      <th>text</th>\n","      <th>clean</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2401</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>im getting on borderlands and i will murder yo...</td>\n","      <td>im getting on borderlands and i will murder yo...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2401</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>I am coming to the borders and I will kill you...</td>\n","      <td>i am coming to the borders and i will kill you...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2401</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>im getting on borderlands and i will kill you ...</td>\n","      <td>im getting on borderlands and i will kill you all</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2401</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>im coming on borderlands and i will murder you...</td>\n","      <td>im coming on borderlands and i will murder you...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2401</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>im getting on borderlands 2 and i will murder ...</td>\n","      <td>im getting on borderlands 2 and i will murder ...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>2401</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>im getting into borderlands and i can murder y...</td>\n","      <td>im getting into borderlands and i can murder y...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>2402</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>So I spent a few hours making something for fu...</td>\n","      <td>so i spent a few hours making something for fu...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>2402</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>So I spent a couple of hours doing something f...</td>\n","      <td>so i spent a couple of hours doing something f...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>2402</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>So I spent a few hours doing something for fun...</td>\n","      <td>so i spent a few hours doing something for fun...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>2402</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>So I spent a few hours making something for fu...</td>\n","      <td>so i spent a few hours making something for fu...</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>2402</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>2010 So I spent a few hours making something f...</td>\n","      <td>2010 so i spent a few hours making something f...</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>2402</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>was</td>\n","      <td>was</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>2403</td>\n","      <td>Borderlands</td>\n","      <td>Neutral</td>\n","      <td>Rock-Hard La Varlope, RARE &amp; POWERFUL, HANDSOM...</td>\n","      <td>rockhard la varlope rare  powerful handsome ja...</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>2403</td>\n","      <td>Borderlands</td>\n","      <td>Neutral</td>\n","      <td>Rock-Hard La Varlope, RARE &amp; POWERFUL, HANDSOM...</td>\n","      <td>rockhard la varlope rare  powerful handsome ja...</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>2403</td>\n","      <td>Borderlands</td>\n","      <td>Neutral</td>\n","      <td>Rock-Hard La Varlope, RARE &amp; POWERFUL, HANDSOM...</td>\n","      <td>rockhard la varlope rare  powerful handsome ja...</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>2403</td>\n","      <td>Borderlands</td>\n","      <td>Neutral</td>\n","      <td>Rock-Hard La Vita, RARE BUT POWERFUL, HANDSOME...</td>\n","      <td>rockhard la vita rare but powerful handsome ja...</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>2403</td>\n","      <td>Borderlands</td>\n","      <td>Neutral</td>\n","      <td>Live Rock - Hard music La la Varlope, RARE &amp; t...</td>\n","      <td>live rock  hard music la la varlope rare  the ...</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>2403</td>\n","      <td>Borderlands</td>\n","      <td>Neutral</td>\n","      <td>I-Hard like me, RARE LONDON DE, HANDSOME 2011,...</td>\n","      <td>ihard like me rare london de handsome 2011 bor...</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>2404</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>that was the first borderlands session in a lo...</td>\n","      <td>that was the first borderlands session in a lo...</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>2404</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>this was the first Borderlands session in a lo...</td>\n","      <td>this was the first borderlands session in a lo...</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>2404</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>that was the first borderlands session in a lo...</td>\n","      <td>that was the first borderlands session in a lo...</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>2404</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>that was the first borderlands session in a lo...</td>\n","      <td>that was the first borderlands session in a lo...</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>2404</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>that I was the first real borderlands session ...</td>\n","      <td>that i was the first real borderlands session ...</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>2404</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>that was the first borderlands session in a ho...</td>\n","      <td>that was the first borderlands session in a ho...</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>2405</td>\n","      <td>Borderlands</td>\n","      <td>Negative</td>\n","      <td>the biggest dissappoinment in my life came out...</td>\n","      <td>the biggest dissappoinment in my life came out...</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>2405</td>\n","      <td>Borderlands</td>\n","      <td>Negative</td>\n","      <td>The biggest disappointment of my life came a y...</td>\n","      <td>the biggest disappointment of my life came a y...</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>2405</td>\n","      <td>Borderlands</td>\n","      <td>Negative</td>\n","      <td>The biggest disappointment of my life came a y...</td>\n","      <td>the biggest disappointment of my life came a y...</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>2405</td>\n","      <td>Borderlands</td>\n","      <td>Negative</td>\n","      <td>the biggest dissappoinment in my life coming o...</td>\n","      <td>the biggest dissappoinment in my life coming o...</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>2405</td>\n","      <td>Borderlands</td>\n","      <td>Negative</td>\n","      <td>For the biggest male dissappoinment in my life...</td>\n","      <td>for the biggest male dissappoinment in my life...</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>2405</td>\n","      <td>Borderlands</td>\n","      <td>Negative</td>\n","      <td>the biggest dissappoinment in my life came bac...</td>\n","      <td>the biggest dissappoinment in my life came bac...</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>2406</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>WE FINISHED BORDERLANDS 3 FINALLY YAS! Thank y...</td>\n","      <td>we finished borderlands 3 finally yas thank yo...</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>2406</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>WE FINALLY FINALLY FIND BORDERLANDS 3 YES! Tha...</td>\n","      <td>we finally finally find borderlands 3 yes than...</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>2406</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>Thank you for hanging up everyone! It was fun....</td>\n","      <td>thank you for hanging up everyone it was fun i...</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>2406</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>WE FINISHED BORDERLANDS 3 UPDATE YAS! Thank yo...</td>\n","      <td>we finished borderlands 3 update yas thank you...</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>2406</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>WE FINISHED BORDERLANDS 3 AND FINALLY YAS! Tha...</td>\n","      <td>we finished borderlands 3 and finally yas than...</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>2406</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>WE FINISHED BORDERLANDS 3 FINALLY YAS! Hey you...</td>\n","      <td>we finished borderlands 3 finally yas hey you ...</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>2407</td>\n","      <td>Borderlands</td>\n","      <td>Negative</td>\n","      <td>Man Gearbox really needs to fix this dissapoin...</td>\n","      <td>man gearbox really needs to fix this dissapoin...</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>2407</td>\n","      <td>Borderlands</td>\n","      <td>Negative</td>\n","      <td>Man Gearbox really needs to fix these disappoi...</td>\n","      <td>man gearbox really needs to fix these disappoi...</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>2407</td>\n","      <td>Borderlands</td>\n","      <td>Negative</td>\n","      <td>Man Gearbox really needs to fix this disssapoi...</td>\n","      <td>man gearbox really needs to fix this disssapoi...</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>2407</td>\n","      <td>Borderlands</td>\n","      <td>Negative</td>\n","      <td>Man Bethesda really needs to fix this dissapoi...</td>\n","      <td>man bethesda really needs to fix this dissapoi...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      id  information      type   \n","0   2401  Borderlands  Positive  \\\n","1   2401  Borderlands  Positive   \n","2   2401  Borderlands  Positive   \n","3   2401  Borderlands  Positive   \n","4   2401  Borderlands  Positive   \n","5   2401  Borderlands  Positive   \n","6   2402  Borderlands  Positive   \n","7   2402  Borderlands  Positive   \n","8   2402  Borderlands  Positive   \n","9   2402  Borderlands  Positive   \n","10  2402  Borderlands  Positive   \n","11  2402  Borderlands  Positive   \n","12  2403  Borderlands   Neutral   \n","13  2403  Borderlands   Neutral   \n","14  2403  Borderlands   Neutral   \n","15  2403  Borderlands   Neutral   \n","16  2403  Borderlands   Neutral   \n","17  2403  Borderlands   Neutral   \n","18  2404  Borderlands  Positive   \n","19  2404  Borderlands  Positive   \n","20  2404  Borderlands  Positive   \n","21  2404  Borderlands  Positive   \n","22  2404  Borderlands  Positive   \n","23  2404  Borderlands  Positive   \n","24  2405  Borderlands  Negative   \n","25  2405  Borderlands  Negative   \n","26  2405  Borderlands  Negative   \n","27  2405  Borderlands  Negative   \n","28  2405  Borderlands  Negative   \n","29  2405  Borderlands  Negative   \n","30  2406  Borderlands  Positive   \n","31  2406  Borderlands  Positive   \n","32  2406  Borderlands  Positive   \n","33  2406  Borderlands  Positive   \n","34  2406  Borderlands  Positive   \n","35  2406  Borderlands  Positive   \n","36  2407  Borderlands  Negative   \n","37  2407  Borderlands  Negative   \n","38  2407  Borderlands  Negative   \n","39  2407  Borderlands  Negative   \n","\n","                                                 text   \n","0   im getting on borderlands and i will murder yo...  \\\n","1   I am coming to the borders and I will kill you...   \n","2   im getting on borderlands and i will kill you ...   \n","3   im coming on borderlands and i will murder you...   \n","4   im getting on borderlands 2 and i will murder ...   \n","5   im getting into borderlands and i can murder y...   \n","6   So I spent a few hours making something for fu...   \n","7   So I spent a couple of hours doing something f...   \n","8   So I spent a few hours doing something for fun...   \n","9   So I spent a few hours making something for fu...   \n","10  2010 So I spent a few hours making something f...   \n","11                                                was   \n","12  Rock-Hard La Varlope, RARE & POWERFUL, HANDSOM...   \n","13  Rock-Hard La Varlope, RARE & POWERFUL, HANDSOM...   \n","14  Rock-Hard La Varlope, RARE & POWERFUL, HANDSOM...   \n","15  Rock-Hard La Vita, RARE BUT POWERFUL, HANDSOME...   \n","16  Live Rock - Hard music La la Varlope, RARE & t...   \n","17  I-Hard like me, RARE LONDON DE, HANDSOME 2011,...   \n","18  that was the first borderlands session in a lo...   \n","19  this was the first Borderlands session in a lo...   \n","20  that was the first borderlands session in a lo...   \n","21  that was the first borderlands session in a lo...   \n","22  that I was the first real borderlands session ...   \n","23  that was the first borderlands session in a ho...   \n","24  the biggest dissappoinment in my life came out...   \n","25  The biggest disappointment of my life came a y...   \n","26  The biggest disappointment of my life came a y...   \n","27  the biggest dissappoinment in my life coming o...   \n","28  For the biggest male dissappoinment in my life...   \n","29  the biggest dissappoinment in my life came bac...   \n","30  WE FINISHED BORDERLANDS 3 FINALLY YAS! Thank y...   \n","31  WE FINALLY FINALLY FIND BORDERLANDS 3 YES! Tha...   \n","32  Thank you for hanging up everyone! It was fun....   \n","33  WE FINISHED BORDERLANDS 3 UPDATE YAS! Thank yo...   \n","34  WE FINISHED BORDERLANDS 3 AND FINALLY YAS! Tha...   \n","35  WE FINISHED BORDERLANDS 3 FINALLY YAS! Hey you...   \n","36  Man Gearbox really needs to fix this dissapoin...   \n","37  Man Gearbox really needs to fix these disappoi...   \n","38  Man Gearbox really needs to fix this disssapoi...   \n","39  Man Bethesda really needs to fix this dissapoi...   \n","\n","                                                clean  \n","0   im getting on borderlands and i will murder yo...  \n","1   i am coming to the borders and i will kill you...  \n","2   im getting on borderlands and i will kill you all  \n","3   im coming on borderlands and i will murder you...  \n","4   im getting on borderlands 2 and i will murder ...  \n","5   im getting into borderlands and i can murder y...  \n","6   so i spent a few hours making something for fu...  \n","7   so i spent a couple of hours doing something f...  \n","8   so i spent a few hours doing something for fun...  \n","9   so i spent a few hours making something for fu...  \n","10  2010 so i spent a few hours making something f...  \n","11                                                was  \n","12  rockhard la varlope rare  powerful handsome ja...  \n","13  rockhard la varlope rare  powerful handsome ja...  \n","14  rockhard la varlope rare  powerful handsome ja...  \n","15  rockhard la vita rare but powerful handsome ja...  \n","16  live rock  hard music la la varlope rare  the ...  \n","17  ihard like me rare london de handsome 2011 bor...  \n","18  that was the first borderlands session in a lo...  \n","19  this was the first borderlands session in a lo...  \n","20  that was the first borderlands session in a lo...  \n","21  that was the first borderlands session in a lo...  \n","22  that i was the first real borderlands session ...  \n","23  that was the first borderlands session in a ho...  \n","24  the biggest dissappoinment in my life came out...  \n","25  the biggest disappointment of my life came a y...  \n","26  the biggest disappointment of my life came a y...  \n","27  the biggest dissappoinment in my life coming o...  \n","28  for the biggest male dissappoinment in my life...  \n","29  the biggest dissappoinment in my life came bac...  \n","30  we finished borderlands 3 finally yas thank yo...  \n","31  we finally finally find borderlands 3 yes than...  \n","32  thank you for hanging up everyone it was fun i...  \n","33  we finished borderlands 3 update yas thank you...  \n","34  we finished borderlands 3 and finally yas than...  \n","35  we finished borderlands 3 finally yas hey you ...  \n","36  man gearbox really needs to fix this dissapoin...  \n","37  man gearbox really needs to fix these disappoi...  \n","38  man gearbox really needs to fix this disssapoi...  \n","39  man bethesda really needs to fix this dissapoi...  "]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["train_data[\"clean\"] = train_data.text.apply(preprocess)\n","test_data[\"clean\"] = test_data.text.apply(preprocess)\n","train_data.head(40)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Feature Engineering"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The feature engineering with clean texts starts from text tokenization, i.e., split the text into word tokens. Let's see what the tokenization do. It groups all the texts by words stored on a list."]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2022-10-16T23:38:28.152525Z","iopub.status.busy":"2022-10-16T23:38:28.152145Z","iopub.status.idle":"2022-10-16T23:38:38.671652Z","shell.execute_reply":"2022-10-16T23:38:38.670297Z","shell.execute_reply.started":"2022-10-16T23:38:28.15249Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["So I spent a few hours making something for fun. . . If you don't know I am a HUGE @Borderlands fan and Maya is one of my favorite characters. So I decided to make myself a wallpaper for my PC. . Here is the original image versus the creation I made :) Enjoy! pic.twitter.com/mLsI5wf9Jg\n","so i spent a few hours making something for fun   if you dont know i am a huge borderlands fan and maya is one of my favorite characters so i decided to make myself a wallpaper for my pc  here is the original image versus the creation i made  enjoy pictwittercommlsi5wf9jg\n","['so', 'i', 'spent', 'a', 'few', 'hours', 'making', 'something', 'for', 'fun', 'if', 'you', 'dont', 'know', 'i', 'am', 'a', 'huge', 'borderlands', 'fan', 'and', 'maya', 'is', 'one', 'of', 'my', 'favorite', 'characters', 'so', 'i', 'decided', 'to', 'make', 'myself', 'a', 'wallpaper', 'for', 'my', 'pc', 'here', 'is', 'the', 'original', 'image', 'versus', 'the', 'creation', 'i', 'made', 'enjoy', 'pictwittercommlsi5wf9jg']\n"]}],"source":["print(train_data.text[6])\n","print(train_data.clean[6])\n","print(word_tokenize(train_data.clean[6]))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We can count the total number of tokens in the training data."]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2022-10-16T23:38:38.673317Z","iopub.status.busy":"2022-10-16T23:38:38.672972Z","iopub.status.idle":"2022-10-16T23:38:38.679953Z","shell.execute_reply":"2022-10-16T23:38:38.67885Z","shell.execute_reply.started":"2022-10-16T23:38:38.673286Z"},"trusted":true},"outputs":[{"data":{"text/plain":["40816"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["len(set(token for text in train_data.clean for token in word_tokenize(text)))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Stop words are the words in a stop list which are filtered out (i.e. stopped) before or after processing of natural language data (text) because they are insignificant. We can refer to `nltk.corpus.stopwords` to obtain the stop words to use."]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2022-10-16T23:38:38.681569Z","iopub.status.busy":"2022-10-16T23:38:38.681194Z","iopub.status.idle":"2022-10-16T23:38:38.694465Z","shell.execute_reply":"2022-10-16T23:38:38.69331Z","shell.execute_reply.started":"2022-10-16T23:38:38.681537Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["179 ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"]}],"source":["stopwords_nltk = nltk.corpus.stopwords\n","stop_words = stopwords_nltk.words('english')\n","print(len(stop_words), stop_words[:10])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["With text tokenization tools, we can conduct feature engineering on texts with stop words filtered out.\n","\n","For simplification, we mainly consider two possible features:\n","- **Word count**: a vector with the dimension of the number of tokens, the value in each dimension is the number of occurrences of the corresponding word token.\n","- **TF-IDF (Term Frequency-Inverse Document Frequency)**: a \"weighted version\" of word count, each value is the term frequency (normalized by total number of tokens) multiplies the inverse of the frequency of documents consisting this word token. For details please refers to https://en.wikipedia.org/wiki/Tf%E2%80%93idf.\n","\n","> **TODO**\n","Sklearn provides the implementation of the two feature extraction methods, and the interface is in the following. **You should manually re-implement at least one of them in our provided framework.**\n","\n","*Hint: if you are concerned about the too large dimension, which might slow the model inference, there are three possible solutions which you can have a try:*\n","- *Hashing: hash the vector into low dimensions with a function from high-dimension index to lower one. We provide a simple hash function and you can also implement a complex one.*\n","- *Random projection: project the vector into low-dimension space with a fixed random projection. We provide a simple projection method.*\n","- *Sparsity: use sparse matrices instead of dense ones, which is used in sklearn's implementation. Please refer to https://docs.scipy.org/doc/scipy/reference/sparse.html for more details.*"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":["class ManualVectorizer(object):\n","    \"\"\"\n","    Manual vectorizer.\n","    \"\"\"\n","    def __init__(self, tokenizer, stop_words):\n","        \"\"\"\n","        Initialize the vectorizer.\n","        You can add additional attributes.\n","        \"\"\"\n","        self.tokenizer = tokenizer\n","        self.stop_words = stop_words\n","        # TODO\n","        self.vocab = None\n","        self.idf = None\n","    def fit_transform(self, texts):\n","        \"\"\"\n","        Fit the dictionary and other attributes (such as IDF) with the texts, and then perform feature extraction.\n","        This method is used on training data.\n","\n","        Parameters:\n","            raw_documents (List[str]): a list of untokenized texts.\n","\n","        Return:\n","            np.array: a 2-D array where each row refers to the feature vector of the corresponding text.\n","        \"\"\"\n","        # TODO\n","        # 统计每个单词在多少个文本中出现过\n","        word_doc_count = Counter() # 创建空的计数器对象\n","        for text in texts:\n","            words = set(self.tokenizer(text)) - set(self.stop_words)\n","            seen_words = set()  # 创建一个空的集合，用于记录已经出现过的单词\n","            for word in words:\n","                if word not in seen_words:  # 如果单词没有出现过，则进行计数\n","                    word_doc_count[word] += 1\n","                    seen_words.add(word)\n","        # 构建词汇表\n","        n = 1000\n","        self.vocab = {word: idx for idx, (word, _) in enumerate(word_doc_count.most_common(n))}\n","        # 计算每个单词的逆文档频率（IDF）\n","        doc_count = len(texts)\n","        self.idf = {word: np.log(doc_count / count) for word, count in word_doc_count.items() if count > 0}\n","        # 对每个文本进行特征提取\n","        features = []\n","        for text in texts:\n","            # 统计每个单词在该文本中出现的频次\n","            words = self.tokenizer(text)\n","            word_counts = Counter(words)\n","            # 构建文本的特征向量\n","            feature_vector = np.zeros(len(self.vocab))\n","            for word, count in word_counts.items():\n","                if word in self.vocab:\n","                    idx = self.vocab[word]\n","                    feature_vector[idx] = count * self.idf[word]\n","            features.append(feature_vector)\n","        # 将特征向量转换为numpy数组并返回\n","        return np.array(features)\n","    \n","    def transform(self, texts):\n","        \"\"\"\n","        Perform feature extraction with the learned dictionary and other attributes.\n","        This method is used on test data.\n","        Note: if a word token does not appear in training data, it will not be counted as the test feature.\n","\n","        Parameters:\n","            raw_documents (List[str]): a list of untokenized texts.\n","\n","        Return:\n","            np.array or np.matrix: a 2-D array where each row refers to the feature vector of the corresponding text.\n","        \"\"\"\n","        # TODO\n","        features = []\n","        for text in texts:\n","            words = self.tokenizer(text)\n","            word_counts = Counter(words)\n","            feature_vector = np.zeros(len(self.vocab))\n","            for word, count in word_counts.items():\n","                if word in self.vocab:\n","                    idx = self.vocab[word]\n","                    feature_vector[idx] = count * self.idf[word]\n","            features.append(feature_vector)\n","        return np.array(features)\n","        "]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":["wordcount_extractor = CountVectorizer(\n","    tokenizer=word_tokenize,\n","    stop_words=stop_words,\n",")\n","\n","tfidf_extractor = TfidfVectorizer(\n","    tokenizer=word_tokenize,\n","    stop_words=stop_words,\n",")\n","\n","manual_extractor = ManualVectorizer(\n","    tokenizer=word_tokenize,\n","    stop_words=stop_words,\n",")\n","extractor = manual_extractor  # TODO: you can modify here"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":["class Identity(object):\n","    \"\"\"\n","    Do nothing.\n","    \"\"\"\n","    def __init__(self):\n","        pass\n","\n","    def __call__(self, xs):\n","        return xs\n","\n","\n","class VectorHasher(object):\n","    \"\"\"\n","    Vector hasher for dimension reduction.\n","    \"\"\"\n","    def __init__(self, target_length=100, hash_func=None):\n","        self.target_length = target_length\n","        if hash_func is None:\n","            self.hash_func = lambda x: x % self.target_length\n","        else:\n","            self.hash_func = hash_func\n","    \n","    def __call__(self, xs):\n","        hashed_xs = np.zeros(xs.shape[:-1] + (self.target_length, ))\n","        for idx in range(xs.shape[-1]):\n","            hashed_idx = self.hash_func(idx)\n","            hashed_xs[:, hashed_idx] += xs[:, idx]\n","        return hashed_xs\n","\n","\n","class VectorProjector(object):\n","    \"\"\"\n","    Vector projector for dimension reduction.\n","    \"\"\"\n","    def __init__(self, source_length, target_length=30):\n","        self.projector = np.random.normal(size=(source_length, target_length))\n","    \n","    def __call__(self, xs):\n","        return xs @ self.projector"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[],"source":["identity = Identity()\n","hasher = VectorHasher()\n","projector = VectorProjector(len(train_data.clean))\n","\n","vector_post_process = VectorHasher()  # TODO: you can modify here"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["X_train_raw = vector_post_process(extractor.fit_transform(train_data.clean))\n","X_test = vector_post_process(extractor.transform(test_data.clean))\n","# print(X_train_raw.shape, X_test.shape)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["For label features, it is natual to assign each label name with an index."]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"data":{"text/plain":["array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0,\n","       0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2,\n","       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0,\n","       0, 0, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["name_to_index = {\n","    \"Positive\": 0,\n","    \"Negative\": 1,\n","    \"Neutral\": 2,\n","    \"Irrelevant\": 3,\n","}\n","y_train_raw = np.asarray(train_data.type.apply(lambda x: name_to_index[x]))\n","y_test = np.asarray(test_data.type.apply(lambda x: name_to_index[x]))\n","y_train_raw[:120]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["---"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Model Selection"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In order to train a model and perform model selection, we should split the raw training data into *training data* and *validation data*\n","> **TODO**\n","\n","**Split the data into training data and validation data with proper ratio. You can use `train_test_split` function.**"]},{"cell_type":"code","execution_count":63,"metadata":{"execution":{"iopub.execute_input":"2022-10-16T23:38:38.7041Z","iopub.status.busy":"2022-10-16T23:38:38.703764Z","iopub.status.idle":"2022-10-16T23:38:38.732995Z","shell.execute_reply":"2022-10-16T23:38:38.731632Z","shell.execute_reply.started":"2022-10-16T23:38:38.70407Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(59745, 100) (14937, 100)\n"]}],"source":["# TODO\n","X_train, X_val, y_train, y_val = train_test_split(X_train_raw, y_train_raw, test_size=0.2, random_state=42)  # modify this line\n","print(X_train.shape, X_val.shape)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now it's time to train your models and select the best ones.\n","\n","> **TODO**\n","\n","**Train your model on `X_train` and `y_train`, and select your model on `X_val` and `y_val`**\n","\n","We provide an example of `DecisionTreeClassifier`. Now it's time for you to select the model you like to conduct classification.\n","\n","> Requirements\n","- Select at least **three** other Machine Learning classification models and train them on the train split. And among them you should implement at least **one** by yourself with our provided interface."]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[],"source":["class ManualModel(object):\n","    \"\"\"\n","    Manual model with sklearn-style interface.\n","    \"\"\"\n","\n","    def __init__(self, n_neighbors=3, metric='manhattan'):\n","        \"\"\"\n","        Initialize the model with some hyperparameters. You can modify the arguments.\n","        \"\"\"\n","        # TODO\n","        self.k = n_neighbors\n","        self.distance_metric = metric\n","        self.X_train = None\n","        self.y_train = None\n","        pass\n","\n","    def fit(self, X, y):\n","        \"\"\"\n","        Fit the model on training set.\n","\n","        Parameters:\n","            X: inputs of training data.\n","            y: labels of training data.\n","        \"\"\"\n","        # TODO\n","        self.X_train = np.array(X)\n","        self.y_train = np.array(y)\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Predict the labels of inputs X with the trained model.\n","\n","        Parameters:\n","            X: inputs of test data\n","\n","        Return:\n","            The predicted labels of test data.\n","        \"\"\"\n","        # TODO\n","        X_test = np.array(X)\n","        predictions = []\n","        for x_test in X_test:\n","            distances = self.calculate_distances(x_test)\n","            nearest_indices = np.argsort(distances)[:self.k]\n","            nearest_labels = self.y_train[nearest_indices]\n","            prediction = Counter(nearest_labels).most_common(1)[0][0]\n","            predictions.append(prediction)\n","        return np.array(predictions)\n","    \n","    def calculate_distances(self, x_test):\n","        \"\"\"\n","        计算测试样本与训练样本之间的距离\n","\n","        Parameters:\n","            x_test: 单个测试样本\n","\n","        Return:\n","            distances: 一个包含x_test与训练样本之间距离的一维数组\n","        \"\"\"\n","        if self.distance_metric == 'euclidean':\n","            distances = np.linalg.norm(self.X_train - x_test, axis=1)\n","        elif self.distance_metric == 'manhattan':\n","            distances = np.sum(np.abs(self.X_train - x_test), axis=1)\n","        elif self.distance_metric == 'minkowski':\n","            p = 2  # Set the power parameter for Minkowski distance\n","            distances = np.power(\n","                np.sum(np.power(np.abs(self.X_train - x_test), p), axis=1), 1/p)\n","        else:\n","            raise ValueError(\n","                \"Invalid distance metric. Please choose from 'euclidean', 'manhattan', 'minkowski', etc.\")\n","        return distances\n"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[],"source":["name_acc_list = {\n","    \"name\": [],\n","    \"acc\": []\n","}\n","\n","def model_assess(model, name='Default'):\n","    model.fit(X_train, y_train)\n","    prds = model.predict(X_val)\n","    acc = 100 * accuracy_score(y_val, prds)\n","    name_acc_list[\"name\"].append(name)\n","    name_acc_list[\"acc\"].append(acc)\n","    print(f'Model: {name}, Accuracy: {acc}%')"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: DT, Accuracy: 70.16134431278034%\n","代码执行时间：2.0940535068511963秒\n"]}],"source":["model_0 = DecisionTreeClassifier(max_depth=None, criterion='entropy')\n","start_time = time.time()\n","model_assess(model_0, \"DT\")\n","end_time = time.time()\n","execution_time = end_time - start_time\n","print(f\"代码执行时间：{execution_time}秒\")"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: DT-1, Accuracy: 79.17921938809668%\n","代码执行时间：439.2210910320282秒\n","Model: DT-2, Accuracy: 41.34029590948651%\n","代码执行时间：0.648491382598877秒\n","Model: DT-3, Accuracy: 81.71654281314856%\n","代码执行时间：60.3113579750061秒\n"]}],"source":["# TODO: add your models here. At least one of them should be your manual model.\n","model_1 = ManualModel(n_neighbors=3, metric='manhattan')\n","start_time = time.time()\n","model_assess(model_1, \"DT-1\")\n","end_time = time.time()\n","execution_time = end_time - start_time\n","print(f\"代码执行时间：{execution_time}秒\")\n","\n","model_2 = LogisticRegression(C=0.001, solver='liblinear', penalty='l2')\n","start_time = time.time()\n","model_assess(model_2, \"DT-2\")\n","end_time = time.time()\n","execution_time = end_time - start_time\n","print(f\"代码执行时间：{execution_time}秒\")\n","\n","model_3 = RandomForestClassifier(max_depth=None, n_estimators=300)\n","start_time = time.time()\n","model_assess(model_3, \"DT-3\")\n","end_time = time.time()\n","execution_time = end_time - start_time\n","print(f\"代码执行时间：{execution_time}秒\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Visualize the model accuracies."]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[],"source":["def plot_acc():\n","    plt.rcParams['figure.figsize']=4,4\n","    sns.set_style('darkgrid')\n","    ax = sns.barplot(x=name_acc_list[\"name\"], y=name_acc_list[\"acc\"], palette=\"coolwarm\", saturation=2.0)\n","    plt.xlabel('Classifier Models', fontsize=12)\n","    plt.ylabel('% of Accuracy', fontsize=12)\n","    plt.title('Accuracy of different Classifier Models', fontsize=16)\n","    plt.xticks(fontsize=12, horizontalalignment='center')\n","    plt.yticks(fontsize=12)\n","    for i in ax.patches:\n","        width, height = i.get_width(), i.get_height()\n","        x, y = i.get_xy() \n","        ax.annotate(f'{round(height,2)}%', (x + width/2, y + height), ha='center', fontsize='x-large')\n","    plt.show()"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[],"source":["# plot_acc()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["> **TODO**\n","\n","**Tune model for good performance on validation set.**\n","\n","**Note:** you should only tune your model on the validation set, and keep the test data **unseen until the model is selected**.\n","\n","Now it is your time to provide the final solution.\n","\n","> Requirements\n","- Tune model's hyperparameters evaluate all your selected models. And give a detailed report on the performance and computational efficiency.\n","- Evaluate your final model on test set, and report the final result.\n","- It is appreciated if other machine learning techniques that help to improve performance are employed."]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[],"source":["# TODO: your code here\n","def perform_grid_search(model, param_grid, X_train, y_train, X_test, y_test, save_path, cv=5):\n","    \"\"\"对于sklearn库中的模型进行网格搜索和交叉验证\n","\n","    Args:\n","        model (_type_): 分类模型\n","        param_grid (_type_): 超参数空间\n","        X_train (_type_): 训练样本特征\n","        y_train (_type_): 训练样本标签\n","        X_test (_type_): 测试样本特征\n","        y_test (_type_): 测试样本标签\n","        save_path (_type_): 保存路径\n","        cv (int, optional): 交叉验证折数\n","    \"\"\"\n","    # 利用GridSearchCV进行网格搜索\n","    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv, scoring='accuracy')\n","\n","    grid_search.fit(X_train, y_train)\n","\n","    # 得到最好的参数组合和最佳模型\n","    best_params = grid_search.best_params_\n","    best_model = grid_search.best_estimator_\n","\n","    # 对于每个参数组合打印准确度\n","    means = grid_search.cv_results_['mean_test_score']\n","    params = grid_search.cv_results_['params']\n","    for mean, param in zip(means, params):\n","        mean_acc = 100 * mean\n","        print(f\"Parameters: {param}, Accuracy: {mean_acc}%\")\n","    print(f\"Best Parameters:{best_params}\")\n","    param_labels = [str(param) for param in params]\n","    mean_accs = [100 * mean for mean in means]\n","    plt.figure(figsize=(10, 6))\n","    plt.bar(param_labels, mean_accs)\n","    plt.xticks(rotation=45)\n","    plt.xlabel('Parameter Combination')\n","    plt.ylabel('Accuracy')\n","    plt.title('Accuracy for Each Parameter Combination')\n","    plt.tight_layout()\n","    \n","    # 获取当前工作目录\n","    current_dir = os.getcwd()\n","    # 拼接保存路径\n","    save_path = os.path.join(current_dir, save_path)\n","    plt.savefig(save_path)\n","    plt.show()\n","    plt.close()\n","    \n","    print(f\"Best Parameters: {best_params}\")\n","    \n","    # 训练最佳模型\n","    best_model.fit(X_train, y_train)\n","\n","    # 在测试集上进行评估\n","    y_pred = best_model.predict(X_test)\n","    accuracy = 100 * accuracy_score(y_test, y_pred)\n","    print(f\"Accuracy on test set: {accuracy}%\")\n","    \n","def knn_grid_search(X, y, param_grid, save_path):\n","    \"\"\"对于手动实现的模型进行网格搜索\n","\n","    Args:\n","        X (_type_): 训练样本特征\n","        y (_type_): 训练样本标签\n","        param_grid (_type_): 超参数空间\n","        save_path (_type_): 保存路径\n","    \"\"\"\n","    params = param_grid['n_neighbors']\n","    metrics = param_grid['metric']\n","    \n","    # 保存不同参数组合的准确度\n","    accuracies = np.zeros((len(params), len(metrics)))\n","    \n","    # 执行K折交叉验证\n","    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n","        \n","    for i, n_neighbors in enumerate(params):\n","        for j, metric in enumerate(metrics):\n","            print(n_neighbors, metric)\n","            knn = ManualModel(n_neighbors=n_neighbors, metric=metric)\n","            knn.fit(X_train, y_train)\n","            y_pred = knn.predict(X_val)\n","            accuracy = 100 * accuracy_score(y_val, y_pred)\n","            accuracies[i, j] += accuracy\n","    \n","    print(accuracies)\n","    # 绘制二维图\n","    fig = plt.figure()\n","    ax = fig.add_subplot(111)\n","    for i in range(len(params)):\n","        ax.plot(metrics, accuracies[i], marker='o', label=f'n_neighbors={params[i]}')\n","    ax.set_xlabel('Metric')\n","    ax.set_ylabel('Accuracy')\n","    ax.legend()\n","    # 获取当前工作目录\n","    current_dir = os.getcwd()\n","    # 拼接保存路径\n","    save_path = os.path.join(current_dir, save_path)\n","    plt.savefig(save_path)\n","    plt.show()\n","    print(accuracies)\n"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[71], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m param_grid_dt \u001b[39m=\u001b[39m {\n\u001b[0;32m      6\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmax_depth\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39mNone\u001b[39;00m],\n\u001b[0;32m      7\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mcriterion\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m'\u001b[39m\u001b[39mgini\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mentropy\u001b[39m\u001b[39m'\u001b[39m]}\n\u001b[0;32m      8\u001b[0m \u001b[39m# 执行交叉验证\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m perform_grid_search(dt, param_grid_dt, X_train_raw, y_train_raw, X_test, y_test, \u001b[39m'\u001b[39;49m\u001b[39mplots/plot1.png\u001b[39;49m\u001b[39m'\u001b[39;49m, cv\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n\u001b[0;32m     11\u001b[0m lr \u001b[39m=\u001b[39m LogisticRegression()\n\u001b[0;32m     12\u001b[0m param_grid_lr \u001b[39m=\u001b[39m {\n\u001b[0;32m     13\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mpenalty\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m'\u001b[39m\u001b[39ml1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39ml2\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     14\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m0.001\u001b[39m, \u001b[39m0.01\u001b[39m, \u001b[39m0.1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m100\u001b[39m],\n\u001b[0;32m     15\u001b[0m     \u001b[39m'\u001b[39m\u001b[39msolver\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m'\u001b[39m\u001b[39mliblinear\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msaga\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     16\u001b[0m }\n","Cell \u001b[1;32mIn[70], line 18\u001b[0m, in \u001b[0;36mperform_grid_search\u001b[1;34m(model, param_grid, X_train, y_train, X_test, y_test, save_path, cv)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m# 利用GridSearchCV进行网格搜索\u001b[39;00m\n\u001b[0;32m     16\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(estimator\u001b[39m=\u001b[39mmodel, param_grid\u001b[39m=\u001b[39mparam_grid, cv\u001b[39m=\u001b[39mcv, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     20\u001b[0m \u001b[39m# 得到最好的参数组合和最佳模型\u001b[39;00m\n\u001b[0;32m     21\u001b[0m best_params \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39mbest_params_\n","File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n","File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n","File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    823\u001b[0m         clone(base_estimator),\n\u001b[0;32m    824\u001b[0m         X,\n\u001b[0;32m    825\u001b[0m         y,\n\u001b[0;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    832\u001b[0m     )\n\u001b[0;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    835\u001b[0m     )\n\u001b[0;32m    836\u001b[0m )\n\u001b[0;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n","File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n","File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n","File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n","File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n","File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n","File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n","File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n","File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    859\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    860\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \n\u001b[0;32m    862\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 889\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    890\u001b[0m         X,\n\u001b[0;32m    891\u001b[0m         y,\n\u001b[0;32m    892\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    893\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n","File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    370\u001b[0m         splitter,\n\u001b[0;32m    371\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    377\u001b[0m     )\n\u001b[1;32m--> 379\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[0;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    382\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["\"\"\"\n","    设置不同的超参数范围，进行交叉验证\n","\"\"\"\n","dt = DecisionTreeClassifier()\n","param_grid_dt = {\n","    'max_depth': [2, 3, 4, None],\n","    'criterion': ['gini', 'entropy']}\n","# 执行交叉验证\n","perform_grid_search(dt, param_grid_dt, X_train_raw, y_train_raw, X_test, y_test, 'plots/plot1.png', cv=5)\n","\n","lr = LogisticRegression()\n","param_grid_lr = {\n","    'penalty': ['l1', 'l2'],\n","    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n","    'solver': ['liblinear', 'saga']\n","}\n","perform_grid_search(lr, param_grid_lr, X_train_raw, y_train_raw, X_test, y_test, 'plots/plot2.png', cv=5)\n","\n","rf = RandomForestClassifier()\n","param_grid_rf = {\n","    'n_estimators': [100, 200, 300],\n","    'max_depth': [5, 10, None]}\n","perform_grid_search(rf, param_grid_rf, X_train_raw, y_train_raw, X_test, y_test, 'plots/plot3.png', cv=5)  \n","\n","param_grid_knn = {\n","    'n_neighbors': [3, 5, 7],\n","    'metric': ['euclidean', 'manhattan', 'minkowski']}\n","knn_grid_search(X_train_raw, y_train_raw, param_grid_knn, 'plots/plot4.png')\n","\n","# knn在测试集上验证\n","best_model = ManualModel(n_neighbors=3, metric='manhattan')\n","best_model.fit(X_train, y_train)\n","prds = best_model.predict(X_test)\n","acc = 100 * accuracy_score(y_test, prds)\n","print(f'Accuracy: {acc}%')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
