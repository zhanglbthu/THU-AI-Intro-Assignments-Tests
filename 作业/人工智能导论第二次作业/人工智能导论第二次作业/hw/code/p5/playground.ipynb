{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Data Mining on Sentiment Analysis"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["---"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Preliminaries"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Import libraries. **You can add other libraries if necessary.**"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-10-16T23:37:03.83871Z","iopub.status.busy":"2022-10-16T23:37:03.837369Z","iopub.status.idle":"2022-10-16T23:37:04.798601Z","shell.execute_reply":"2022-10-16T23:37:04.79744Z","shell.execute_reply.started":"2022-10-16T23:37:03.838618Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer  # Data transformation\n","from sklearn.model_selection import train_test_split  # Data testing\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score  # Comparison between real and predicted\n","import re  # Regular expressions\n","import nltk\n","from nltk import word_tokenize\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","from collections import Counter\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import GridSearchCV # 交叉验证\n","import os\n","import time"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Load the data and add column keys."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-16T23:37:04.804857Z","iopub.status.busy":"2022-10-16T23:37:04.804516Z","iopub.status.idle":"2022-10-16T23:37:05.043216Z","shell.execute_reply":"2022-10-16T23:37:05.042013Z","shell.execute_reply.started":"2022-10-16T23:37:04.804825Z"},"trusted":true},"outputs":[],"source":["train_data = pd.read_csv(\"data/sentiment_train.csv\", header=None)\n","test_data = pd.read_csv(\"data/sentiment_test.csv\", header=None)\n","train_data.columns = ['id', 'information', 'type', 'text']\n","test_data.columns = ['id', 'information', 'type', 'text']"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Take a glance at the provided data."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-16T23:37:05.044993Z","iopub.status.busy":"2022-10-16T23:37:05.044632Z","iopub.status.idle":"2022-10-16T23:37:05.062473Z","shell.execute_reply":"2022-10-16T23:37:05.06133Z","shell.execute_reply.started":"2022-10-16T23:37:05.044962Z"},"trusted":true},"outputs":[],"source":["train_data.info()\n","train_data.head(40)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Each item consists of 4 columns, where Columns ID and Information are almost task-irrevalent. **Column Type is users' sentiments, which we should predict as our labels based on Column Text.**\n","Now let's take a look at possible values of Column Type."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data[\"type\"].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_data[\"type\"].value_counts()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["There are 4 possible values, and ***our goal is to perform the quadruple classification over texts.***"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["---"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Data Processing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def preprocess(text):\n","    # TODO\n","    # 将输入文本转换为字符串类型\n","    text = str(text)\n","    # 将所有字母转换为小写字母\n","    text = text.lower()\n","    # 过滤文本中的标点符号和空格\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    return text  # modify this line"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Perform preprocessing, and compare the raw text and the preprocessed tokens."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-16T23:37:05.121422Z","iopub.status.busy":"2022-10-16T23:37:05.121009Z","iopub.status.idle":"2022-10-16T23:37:05.59644Z","shell.execute_reply":"2022-10-16T23:37:05.595292Z","shell.execute_reply.started":"2022-10-16T23:37:05.121385Z"},"trusted":true},"outputs":[],"source":["train_data[\"clean\"] = train_data.text.apply(preprocess)\n","test_data[\"clean\"] = test_data.text.apply(preprocess)\n","train_data.head(40)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Feature Engineering"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The feature engineering with clean texts starts from text tokenization, i.e., split the text into word tokens. Let's see what the tokenization do. It groups all the texts by words stored on a list."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-16T23:38:28.152525Z","iopub.status.busy":"2022-10-16T23:38:28.152145Z","iopub.status.idle":"2022-10-16T23:38:38.671652Z","shell.execute_reply":"2022-10-16T23:38:38.670297Z","shell.execute_reply.started":"2022-10-16T23:38:28.15249Z"},"trusted":true},"outputs":[],"source":["print(train_data.text[6])\n","print(train_data.clean[6])\n","print(word_tokenize(train_data.clean[6]))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We can count the total number of tokens in the training data."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-16T23:38:38.673317Z","iopub.status.busy":"2022-10-16T23:38:38.672972Z","iopub.status.idle":"2022-10-16T23:38:38.679953Z","shell.execute_reply":"2022-10-16T23:38:38.67885Z","shell.execute_reply.started":"2022-10-16T23:38:38.673286Z"},"trusted":true},"outputs":[],"source":["len(set(token for text in train_data.clean for token in word_tokenize(text)))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Stop words are the words in a stop list which are filtered out (i.e. stopped) before or after processing of natural language data (text) because they are insignificant. We can refer to `nltk.corpus.stopwords` to obtain the stop words to use."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-16T23:38:38.681569Z","iopub.status.busy":"2022-10-16T23:38:38.681194Z","iopub.status.idle":"2022-10-16T23:38:38.694465Z","shell.execute_reply":"2022-10-16T23:38:38.69331Z","shell.execute_reply.started":"2022-10-16T23:38:38.681537Z"},"trusted":true},"outputs":[],"source":["stopwords_nltk = nltk.corpus.stopwords\n","stop_words = stopwords_nltk.words('english')\n","print(len(stop_words), stop_words[:10])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["With text tokenization tools, we can conduct feature engineering on texts with stop words filtered out.\n","\n","For simplification, we mainly consider two possible features:\n","- **Word count**: a vector with the dimension of the number of tokens, the value in each dimension is the number of occurrences of the corresponding word token.\n","- **TF-IDF (Term Frequency-Inverse Document Frequency)**: a \"weighted version\" of word count, each value is the term frequency (normalized by total number of tokens) multiplies the inverse of the frequency of documents consisting this word token. For details please refers to https://en.wikipedia.org/wiki/Tf%E2%80%93idf.\n","\n","> **TODO**\n","Sklearn provides the implementation of the two feature extraction methods, and the interface is in the following. **You should manually re-implement at least one of them in our provided framework.**\n","\n","*Hint: if you are concerned about the too large dimension, which might slow the model inference, there are three possible solutions which you can have a try:*\n","- *Hashing: hash the vector into low dimensions with a function from high-dimension index to lower one. We provide a simple hash function and you can also implement a complex one.*\n","- *Random projection: project the vector into low-dimension space with a fixed random projection. We provide a simple projection method.*\n","- *Sparsity: use sparse matrices instead of dense ones, which is used in sklearn's implementation. Please refer to https://docs.scipy.org/doc/scipy/reference/sparse.html for more details.*"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ManualVectorizer(object):\n","    \"\"\"\n","    Manual vectorizer.\n","    \"\"\"\n","    def __init__(self, tokenizer, stop_words):\n","        \"\"\"\n","        Initialize the vectorizer.\n","        You can add additional attributes.\n","        \"\"\"\n","        self.tokenizer = tokenizer\n","        self.stop_words = stop_words\n","        # TODO\n","        self.vocab = None\n","        self.idf = None\n","    def fit_transform(self, texts):\n","        \"\"\"\n","        Fit the dictionary and other attributes (such as IDF) with the texts, and then perform feature extraction.\n","        This method is used on training data.\n","\n","        Parameters:\n","            raw_documents (List[str]): a list of untokenized texts.\n","\n","        Return:\n","            np.array: a 2-D array where each row refers to the feature vector of the corresponding text.\n","        \"\"\"\n","        # TODO\n","        # 统计每个单词在多少个文本中出现过\n","        word_doc_count = Counter() # 创建空的计数器对象\n","        for text in texts:\n","            words = set(self.tokenizer(text)) - set(self.stop_words)\n","            seen_words = set()  # 创建一个空的集合，用于记录已经出现过的单词\n","            for word in words:\n","                if word not in seen_words:  # 如果单词没有出现过，则进行计数\n","                    word_doc_count[word] += 1\n","                    seen_words.add(word)\n","        # 构建词汇表\n","        n = 1000\n","        self.vocab = {word: idx for idx, (word, _) in enumerate(word_doc_count.most_common(n))}\n","        # 计算每个单词的逆文档频率（IDF）\n","        doc_count = len(texts)\n","        self.idf = {word: np.log(doc_count / count) for word, count in word_doc_count.items() if count > 0}\n","        # 对每个文本进行特征提取\n","        features = []\n","        for text in texts:\n","            # 统计每个单词在该文本中出现的频次\n","            words = self.tokenizer(text)\n","            word_counts = Counter(words)\n","            # 构建文本的特征向量\n","            feature_vector = np.zeros(len(self.vocab))\n","            for word, count in word_counts.items():\n","                if word in self.vocab:\n","                    idx = self.vocab[word]\n","                    feature_vector[idx] = count * self.idf[word]\n","            features.append(feature_vector)\n","        # 将特征向量转换为numpy数组并返回\n","        return np.array(features)\n","    \n","    def transform(self, texts):\n","        \"\"\"\n","        Perform feature extraction with the learned dictionary and other attributes.\n","        This method is used on test data.\n","        Note: if a word token does not appear in training data, it will not be counted as the test feature.\n","\n","        Parameters:\n","            raw_documents (List[str]): a list of untokenized texts.\n","\n","        Return:\n","            np.array or np.matrix: a 2-D array where each row refers to the feature vector of the corresponding text.\n","        \"\"\"\n","        # TODO\n","        features = []\n","        for text in texts:\n","            words = self.tokenizer(text)\n","            word_counts = Counter(words)\n","            feature_vector = np.zeros(len(self.vocab))\n","            for word, count in word_counts.items():\n","                if word in self.vocab:\n","                    idx = self.vocab[word]\n","                    feature_vector[idx] = count * self.idf[word]\n","            features.append(feature_vector)\n","        return np.array(features)\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["wordcount_extractor = CountVectorizer(\n","    tokenizer=word_tokenize,\n","    stop_words=stop_words,\n",")\n","\n","tfidf_extractor = TfidfVectorizer(\n","    tokenizer=word_tokenize,\n","    stop_words=stop_words,\n",")\n","\n","manual_extractor = ManualVectorizer(\n","    tokenizer=word_tokenize,\n","    stop_words=stop_words,\n",")\n","extractor = manual_extractor  # TODO: you can modify here"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Identity(object):\n","    \"\"\"\n","    Do nothing.\n","    \"\"\"\n","    def __init__(self):\n","        pass\n","\n","    def __call__(self, xs):\n","        return xs\n","\n","\n","class VectorHasher(object):\n","    \"\"\"\n","    Vector hasher for dimension reduction.\n","    \"\"\"\n","    def __init__(self, target_length=100, hash_func=None):\n","        self.target_length = target_length\n","        if hash_func is None:\n","            self.hash_func = lambda x: x % self.target_length\n","        else:\n","            self.hash_func = hash_func\n","    \n","    def __call__(self, xs):\n","        hashed_xs = np.zeros(xs.shape[:-1] + (self.target_length, ))\n","        for idx in range(xs.shape[-1]):\n","            hashed_idx = self.hash_func(idx)\n","            hashed_xs[:, hashed_idx] += xs[:, idx]\n","        return hashed_xs\n","\n","\n","class VectorProjector(object):\n","    \"\"\"\n","    Vector projector for dimension reduction.\n","    \"\"\"\n","    def __init__(self, source_length, target_length=30):\n","        self.projector = np.random.normal(size=(source_length, target_length))\n","    \n","    def __call__(self, xs):\n","        return xs @ self.projector"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["identity = Identity()\n","hasher = VectorHasher()\n","projector = VectorProjector(len(train_data.clean))\n","\n","vector_post_process = VectorHasher()  # TODO: you can modify here"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train_raw = vector_post_process(extractor.fit_transform(train_data.clean))\n","X_test = vector_post_process(extractor.transform(test_data.clean))\n","# print(X_train_raw.shape, X_test.shape)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["For label features, it is natual to assign each label name with an index."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["name_to_index = {\n","    \"Positive\": 0,\n","    \"Negative\": 1,\n","    \"Neutral\": 2,\n","    \"Irrelevant\": 3,\n","}\n","y_train_raw = np.asarray(train_data.type.apply(lambda x: name_to_index[x]))\n","y_test = np.asarray(test_data.type.apply(lambda x: name_to_index[x]))\n","y_train_raw[:120]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["---"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Model Selection"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In order to train a model and perform model selection, we should split the raw training data into *training data* and *validation data*\n","> **TODO**\n","\n","**Split the data into training data and validation data with proper ratio. You can use `train_test_split` function.**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-16T23:38:38.7041Z","iopub.status.busy":"2022-10-16T23:38:38.703764Z","iopub.status.idle":"2022-10-16T23:38:38.732995Z","shell.execute_reply":"2022-10-16T23:38:38.731632Z","shell.execute_reply.started":"2022-10-16T23:38:38.70407Z"},"trusted":true},"outputs":[],"source":["# TODO\n","X_train, X_val, y_train, y_val = train_test_split(X_train_raw, y_train_raw, test_size=0.2, random_state=42)  # modify this line\n","print(X_train.shape, X_val.shape)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now it's time to train your models and select the best ones.\n","\n","> **TODO**\n","\n","**Train your model on `X_train` and `y_train`, and select your model on `X_val` and `y_val`**\n","\n","We provide an example of `DecisionTreeClassifier`. Now it's time for you to select the model you like to conduct classification.\n","\n","> Requirements\n","- Select at least **three** other Machine Learning classification models and train them on the train split. And among them you should implement at least **one** by yourself with our provided interface."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ManualModel(object):\n","    \"\"\"\n","    Manual model with sklearn-style interface.\n","    \"\"\"\n","\n","    def __init__(self, n_neighbors=3, metric='manhattan'):\n","        \"\"\"\n","        Initialize the model with some hyperparameters. You can modify the arguments.\n","        \"\"\"\n","        # TODO\n","        self.k = n_neighbors\n","        self.distance_metric = metric\n","        self.X_train = None\n","        self.y_train = None\n","        pass\n","\n","    def fit(self, X, y):\n","        \"\"\"\n","        Fit the model on training set.\n","\n","        Parameters:\n","            X: inputs of training data.\n","            y: labels of training data.\n","        \"\"\"\n","        # TODO\n","        self.X_train = np.array(X)\n","        self.y_train = np.array(y)\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Predict the labels of inputs X with the trained model.\n","\n","        Parameters:\n","            X: inputs of test data\n","\n","        Return:\n","            The predicted labels of test data.\n","        \"\"\"\n","        # TODO\n","        X_test = np.array(X)\n","        predictions = []\n","        for x_test in X_test:\n","            distances = self.calculate_distances(x_test)\n","            nearest_indices = np.argsort(distances)[:self.k]\n","            nearest_labels = self.y_train[nearest_indices]\n","            prediction = Counter(nearest_labels).most_common(1)[0][0]\n","            predictions.append(prediction)\n","        return np.array(predictions)\n","    \n","    def calculate_distances(self, x_test):\n","        \"\"\"\n","        计算测试样本与训练样本之间的距离\n","\n","        Parameters:\n","            x_test: 单个测试样本\n","\n","        Return:\n","            distances: 一个包含x_test与训练样本之间距离的一维数组\n","        \"\"\"\n","        if self.distance_metric == 'euclidean':\n","            distances = np.linalg.norm(self.X_train - x_test, axis=1)\n","        elif self.distance_metric == 'manhattan':\n","            distances = np.sum(np.abs(self.X_train - x_test), axis=1)\n","        elif self.distance_metric == 'minkowski':\n","            p = 2  # Set the power parameter for Minkowski distance\n","            distances = np.power(\n","                np.sum(np.power(np.abs(self.X_train - x_test), p), axis=1), 1/p)\n","        else:\n","            raise ValueError(\n","                \"Invalid distance metric. Please choose from 'euclidean', 'manhattan', 'minkowski', etc.\")\n","        return distances\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["name_acc_list = {\n","    \"name\": [],\n","    \"acc\": []\n","}\n","\n","def model_assess(model, name='Default'):\n","    model.fit(X_train, y_train)\n","    prds = model.predict(X_val)\n","    acc = 100 * accuracy_score(y_val, prds)\n","    name_acc_list[\"name\"].append(name)\n","    name_acc_list[\"acc\"].append(acc)\n","    print(f'Model: {name}, Accuracy: {acc}%')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_0 = DecisionTreeClassifier(max_depth=None, criterion='entropy')\n","start_time = time.time()\n","model_assess(model_0, \"DT\")\n","end_time = time.time()\n","execution_time = end_time - start_time\n","print(f\"代码执行时间：{execution_time}秒\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TODO: add your models here. At least one of them should be your manual model.\n","model_1 = ManualModel(n_neighbors=3, metric='manhattan')\n","start_time = time.time()\n","model_assess(model_1, \"DT-1\")\n","end_time = time.time()\n","execution_time = end_time - start_time\n","print(f\"代码执行时间：{execution_time}秒\")\n","\n","model_2 = LogisticRegression(C=0.001, solver='liblinear', penalty='l2')\n","start_time = time.time()\n","model_assess(model_2, \"DT-2\")\n","end_time = time.time()\n","execution_time = end_time - start_time\n","print(f\"代码执行时间：{execution_time}秒\")\n","\n","model_3 = RandomForestClassifier(max_depth=None, n_estimators=300)\n","start_time = time.time()\n","model_assess(model_3, \"DT-3\")\n","end_time = time.time()\n","execution_time = end_time - start_time\n","print(f\"代码执行时间：{execution_time}秒\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Visualize the model accuracies."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_acc():\n","    plt.rcParams['figure.figsize']=4,4\n","    sns.set_style('darkgrid')\n","    ax = sns.barplot(x=name_acc_list[\"name\"], y=name_acc_list[\"acc\"], palette=\"coolwarm\", saturation=2.0)\n","    plt.xlabel('Classifier Models', fontsize=12)\n","    plt.ylabel('% of Accuracy', fontsize=12)\n","    plt.title('Accuracy of different Classifier Models', fontsize=16)\n","    plt.xticks(fontsize=12, horizontalalignment='center')\n","    plt.yticks(fontsize=12)\n","    for i in ax.patches:\n","        width, height = i.get_width(), i.get_height()\n","        x, y = i.get_xy() \n","        ax.annotate(f'{round(height,2)}%', (x + width/2, y + height), ha='center', fontsize='x-large')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# plot_acc()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["> **TODO**\n","\n","**Tune model for good performance on validation set.**\n","\n","**Note:** you should only tune your model on the validation set, and keep the test data **unseen until the model is selected**.\n","\n","Now it is your time to provide the final solution.\n","\n","> Requirements\n","- Tune model's hyperparameters evaluate all your selected models. And give a detailed report on the performance and computational efficiency.\n","- Evaluate your final model on test set, and report the final result.\n","- It is appreciated if other machine learning techniques that help to improve performance are employed."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TODO: your code here\n","def perform_grid_search(model, param_grid, X_train, y_train, X_test, y_test, save_path, cv=5):\n","    \"\"\"对于sklearn库中的模型进行网格搜索和交叉验证\n","\n","    Args:\n","        model (_type_): 分类模型\n","        param_grid (_type_): 超参数空间\n","        X_train (_type_): 训练样本特征\n","        y_train (_type_): 训练样本标签\n","        X_test (_type_): 测试样本特征\n","        y_test (_type_): 测试样本标签\n","        save_path (_type_): 保存路径\n","        cv (int, optional): 交叉验证折数\n","    \"\"\"\n","    # 利用GridSearchCV进行网格搜索\n","    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv, scoring='accuracy')\n","\n","    grid_search.fit(X_train, y_train)\n","\n","    # 得到最好的参数组合和最佳模型\n","    best_params = grid_search.best_params_\n","    best_model = grid_search.best_estimator_\n","\n","    # 对于每个参数组合打印准确度\n","    means = grid_search.cv_results_['mean_test_score']\n","    params = grid_search.cv_results_['params']\n","    for mean, param in zip(means, params):\n","        mean_acc = 100 * mean\n","        print(f\"Parameters: {param}, Accuracy: {mean_acc}%\")\n","    print(f\"Best Parameters:{best_params}\")\n","    param_labels = [str(param) for param in params]\n","    mean_accs = [100 * mean for mean in means]\n","    plt.figure(figsize=(10, 6))\n","    plt.bar(param_labels, mean_accs)\n","    plt.xticks(rotation=45)\n","    plt.xlabel('Parameter Combination')\n","    plt.ylabel('Accuracy')\n","    plt.title('Accuracy for Each Parameter Combination')\n","    plt.tight_layout()\n","    \n","    # 获取当前工作目录\n","    current_dir = os.getcwd()\n","    # 拼接保存路径\n","    save_path = os.path.join(current_dir, save_path)\n","    plt.savefig(save_path)\n","    plt.show()\n","    plt.close()\n","    \n","    print(f\"Best Parameters: {best_params}\")\n","    \n","    # 训练最佳模型\n","    best_model.fit(X_train, y_train)\n","\n","    # 在测试集上进行评估\n","    y_pred = best_model.predict(X_test)\n","    accuracy = 100 * accuracy_score(y_test, y_pred)\n","    print(f\"Accuracy on test set: {accuracy}%\")\n","    \n","def knn_grid_search(X, y, param_grid, save_path):\n","    \"\"\"对于手动实现的模型进行网格搜索\n","\n","    Args:\n","        X (_type_): 训练样本特征\n","        y (_type_): 训练样本标签\n","        param_grid (_type_): 超参数空间\n","        save_path (_type_): 保存路径\n","    \"\"\"\n","    params = param_grid['n_neighbors']\n","    metrics = param_grid['metric']\n","    \n","    # 保存不同参数组合的准确度\n","    accuracies = np.zeros((len(params), len(metrics)))\n","    \n","    # 执行K折交叉验证\n","    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n","        \n","    for i, n_neighbors in enumerate(params):\n","        for j, metric in enumerate(metrics):\n","            print(n_neighbors, metric)\n","            knn = ManualModel(n_neighbors=n_neighbors, metric=metric)\n","            knn.fit(X_train, y_train)\n","            y_pred = knn.predict(X_val)\n","            accuracy = 100 * accuracy_score(y_val, y_pred)\n","            accuracies[i, j] += accuracy\n","    \n","    print(accuracies)\n","    # 绘制二维图\n","    fig = plt.figure()\n","    ax = fig.add_subplot(111)\n","    for i in range(len(params)):\n","        ax.plot(metrics, accuracies[i], marker='o', label=f'n_neighbors={params[i]}')\n","    ax.set_xlabel('Metric')\n","    ax.set_ylabel('Accuracy')\n","    ax.legend()\n","    # 获取当前工作目录\n","    current_dir = os.getcwd()\n","    # 拼接保存路径\n","    save_path = os.path.join(current_dir, save_path)\n","    plt.savefig(save_path)\n","    plt.show()\n","    print(accuracies)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","    设置不同的超参数范围，进行交叉验证\n","\"\"\"\n","dt = DecisionTreeClassifier()\n","param_grid_dt = {\n","    'max_depth': [2, 3, 4, None],\n","    'criterion': ['gini', 'entropy']}\n","# 执行交叉验证\n","perform_grid_search(dt, param_grid_dt, X_train_raw, y_train_raw, X_test, y_test, 'plots/plot1.png', cv=5)\n","\n","lr = LogisticRegression()\n","param_grid_lr = {\n","    'penalty': ['l1', 'l2'],\n","    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n","    'solver': ['liblinear', 'saga']\n","}\n","perform_grid_search(lr, param_grid_lr, X_train_raw, y_train_raw, X_test, y_test, 'plots/plot2.png', cv=5)\n","\n","rf = RandomForestClassifier()\n","param_grid_rf = {\n","    'n_estimators': [100, 200, 300],\n","    'max_depth': [5, 10, None]}\n","perform_grid_search(rf, param_grid_rf, X_train_raw, y_train_raw, X_test, y_test, 'plots/plot3.png', cv=5)  \n","\n","param_grid_knn = {\n","    'n_neighbors': [3, 5, 7],\n","    'metric': ['euclidean', 'manhattan', 'minkowski']}\n","knn_grid_search(X_train_raw, y_train_raw, param_grid_knn, 'plots/plot4.png')\n","\n","# knn在测试集上验证\n","best_model = ManualModel(n_neighbors=3, metric='manhattan')\n","best_model.fit(X_train, y_train)\n","prds = best_model.predict(X_test)\n","acc = 100 * accuracy_score(y_test, prds)\n","print(f'Accuracy: {acc}%')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
